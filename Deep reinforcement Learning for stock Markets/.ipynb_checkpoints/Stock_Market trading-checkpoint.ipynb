{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFuqD8-pqo21"
   },
   "outputs": [],
   "source": [
    "    !pip install tensorflow-gpu==2.4.1\n",
    "     \n",
    "    !pip install pandas-datareader\n",
    "     \n",
    "    import math\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas_datareader as data_reader\n",
    "     \n",
    "    from tqdm import tqdm_notebook, tqdm\n",
    "    from collections import deque\n",
    "     \n",
    "    class AITrader:\n",
    "      def __init__(self, state_size, action_space=3, model_name=\"AITrader\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_space = action_space # Stay, Buy, Sell\n",
    "        # A type of linked list \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "     \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "     \n",
    "        # A compiled Sequential model from Keras\n",
    "        self.model = self.model_builder()\n",
    "     \n",
    "      # The brain\n",
    "     \n",
    "      # Creating the model\n",
    "      def model_builder(self):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(tf.keras.layers.Dense(units=32, activation='relu', input_dim=self.state_size))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "     \n",
    "        # Output layer\n",
    "        # Units = number of classes\n",
    "        model.add(tf.keras.layers.Dense(units=self.action_space, activation='linear'))\n",
    "     \n",
    "        # Compiling the model\n",
    "        model.compile(\n",
    "          loss='mse',\n",
    "          optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        )\n",
    "        return model\n",
    "     \n",
    "      def trade(self, state):\n",
    "        # In this case: random action - epsilon will decrease\n",
    "        if random.random() <= self.epsilon:\n",
    "          return random.randrange(self.action_space) # Random choice of one of our possible actions\n",
    "        \n",
    "        actions = self.model.predict(state)\n",
    "        return np.argmax(actions[0])\n",
    "     \n",
    "      def batch_train(self, batch_size):\n",
    "        batch = []\n",
    "        # Starting from memory minus batch size until the latest point in memory\n",
    "        for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
    "          # Collect all these objects in our memory Linked List\n",
    "          batch.append(self.memory[i])\n",
    "        \n",
    "        # We could condense the last for loop and this, potentially\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "          # This line is not useful\n",
    "          reward = reward\n",
    "          if not done:\n",
    "            # Q(s, a) = R(s, a) + gamma * V(s')\n",
    "            temp = self.model.predict(next_state)[0]\n",
    "            print(f\"temp: {temp}\")\n",
    "            reward = reward + (self.gamma * np.amax(temp))\n",
    "          \n",
    "          target = self.model.predict(state)\n",
    "          print(f\"target: {target}\")\n",
    "          target[0][action] = reward\n",
    "          self.model.fit(state, target, epochs=1, verbose=0)\n",
    "     \n",
    "          # Decrease epsilon - our randomness - based on our learning rate (0.005)\n",
    "          if self.epsilon > self.epsilon_final:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "     \n",
    "    # Helper functions\n",
    "    def sigmoid(x):\n",
    "      \"\"\"\n",
    "        Parameters: number (integer)\n",
    "        Returns: Float (number after a sigmoid interface)\n",
    "      \"\"\"\n",
    "      return 1 / (1 + math.exp(-x))\n",
    "     \n",
    "    def stocks_price_format(n):\n",
    "      \"\"\"\n",
    "        Parameters: n - int\n",
    "        Returns: string\n",
    "      \"\"\"\n",
    "      if n < 0:\n",
    "        return f\"- $ {abs(n):0.2f}\"\n",
    "      else:\n",
    "        return f\"$ {abs(n):0.2f}\"\n",
    "     \n",
    "    def dataset_loader(stock_name):\n",
    "      \"\"\"\n",
    "        Parameters: string\n",
    "        return: Pandas Data Series\n",
    "      \"\"\"\n",
    "      # Dataset is a Pandas Dataframe\n",
    "      dataset = data_reader.DataReader(stock_name, data_source = \"yahoo\")\n",
    "     \n",
    "      # This is data we don't actually use, but it could be useful\n",
    "      # This is the start and end date of the stock data\n",
    "      start_date = str(dataset.index[0]).split()[0]\n",
    "      end_date = str(dataset.index[-1]).split()[0]\n",
    "     \n",
    "      close = dataset['Close']\n",
    "      return close\n",
    "     \n",
    "    def state_creator(data, timestep, window_size):\n",
    "      \"\"\"\n",
    "        Parameters:\n",
    "          data: our Pandas Series\n",
    "          timestep: How many days at a time we take - int\n",
    "          window_size: int\n",
    "      \"\"\"\n",
    "      # Identify the row which will be the starting point in our Pandas Series\n",
    "      starting_id = timestep - window_size + 1\n",
    "      if starting_id >= 0:\n",
    "        # If it's a positive number, then we'll get the rows in our series from\n",
    "        # our starting point until the length specified in our timestep\n",
    "        windowed_data = data[starting_id:timestep+1]\n",
    "      else:\n",
    "        # Else starting_id is negative, so we create a list\n",
    "        # That's as long as the starting ID then it gets all the items from the\n",
    "        # beginning of the list and appends those to the end\n",
    "        temp = [data[0]]\n",
    "        print(f\"[data[0]]: {temp}\")\n",
    "        print(f\"type: {type(temp)}\")\n",
    "        temp *= -starting_id\n",
    "        print(f\"*= -starting_id: {temp}\")\n",
    "        print(f\"type: {type(temp)}\")\n",
    "        temp_1 = list(data[0:timestep+1])\n",
    "        print(f\"list(data[0:timestep+1]: {temp_1}\")\n",
    "        print(f\"type: {type(temp_1)}\")\n",
    "        windowed_data = -starting_id * [data[0]] + list(data[0:timestep+1])\n",
    "      \n",
    "      state = []\n",
    "      print(f\"windowed_data: {windowed_data}\")\n",
    "      for i in range(window_size - 1):\n",
    "        # For each item, create a list (that will become a np array)\n",
    "        # Of the sigmoid function with the different between the next\n",
    "        # item in our list and the current\n",
    "        # N.B.: In this step we're ALWAYS converting it to a list\n",
    "        state.append(sigmoid(windowed_data[i+1] - windowed_data[i]))\n",
    "      \n",
    "      print(f\"state:\")\n",
    "      print(state)\n",
    "      print(type(state))\n",
    "      return np.array([state])\n",
    "     \n",
    "    stock_name = 'AAPL'\n",
    "    data = dataset_loader(stock_name)\n",
    "     \n",
    "    print(data)\n",
    "     \n",
    "    # These were for me to learn, given all the comments and print statements I added to state_creator, so I could understand what the possible outcomes were\n",
    "    state_creator(data, 0, 11)\n",
    "    state_creator(data, 20, 11)\n",
    "     \n",
    "    window_size = 10\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "    data_samples = len(data) - 1\n",
    "     \n",
    "    trader = AITrader(window_size)\n",
    "     \n",
    "    trader.model.summary()\n",
    "     \n",
    "    for episode in range(episodes + 1):\n",
    "      print(f\"Episode: {episode}/{episodes}\")\n",
    "      # On the first iteration:\n",
    "      # Our state will be window_size + 1 (11) list of copies of the\n",
    "      # close value of the stock\n",
    "      state = state_creator(data, 0, window_size + 1)\n",
    "      total_profit = 0\n",
    "      trader.inventory = []\n",
    "      # Range: 1259 + 1\n",
    "      # A tqdm series is like any other iterable except it creates nice print statements as we proceed\n",
    "      for t in tqdm(range(data_samples)):\n",
    "        # What will our next action be?\n",
    "        action = trader.trade(state)\n",
    "        # Next state: What is our next state?\n",
    "        # On the first iteration it will be window_size + 1 copies of the\n",
    "        # close value of the stock PLUS the stock value at close on day + 1\n",
    "     \n",
    "        # When t+1 finally becomes larger than 11\n",
    "        # We'll just get the sigmoid'd values of the stock in that range\n",
    "        next_state = state_creator(data, t+1, window_size + 1)\n",
    "        reward = 0\n",
    "     \n",
    "        if action == 1: # Buying\n",
    "          trader.inventory.append(data[t])\n",
    "          print(\"AI Trader bought: \", stocks_price_format(data[t]))\n",
    "          \n",
    "        elif action == 2 and len(trader.inventory) > 0: # Selling\n",
    "          # N.B.: Selling can only occur if trader has anything to sell\n",
    "          buy_price = trader.inventory.pop(0)\n",
    "          # The buy price is the last price that the trader got\n",
    "          reward = max(data[t] - buy_price, 0)\n",
    "          # The reward is how much profit we made\n",
    "          total_profit += data[t] - buy_price\n",
    "          # Our profit goes up by that amount\n",
    "          print(\"AI Trader sold: \", stocks_price_format(data[t]), \" Profit: \" + stocks_price_format(data[t] - buy_price) )\n",
    "     \n",
    "        # If we run out of samples to examine!\n",
    "        if t == data_samples - 1:\n",
    "          done = True\n",
    "        else:\n",
    "          done = False\n",
    "     \n",
    "        # We add these to the memory so that we can do some batch trading\n",
    "        trader.memory.append((state, action, reward, next_state, done))\n",
    "        # Now the date has moved forward, and we want our current state to be\n",
    "        # the pfuture state\n",
    "        state = next_state\n",
    "     \n",
    "        if done:\n",
    "          print(\"########################\")\n",
    "          print(\"TOTAL PROFIT: {}\".format(total_profit))\n",
    "          print(\"########################\")\n",
    "        \n",
    "        if len(trader.memory) > batch_size:\n",
    "          trader.batch_train(batch_size)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0UZ6rxn2kpWcQXPWWGLUC",
   "name": "Stock_Market trading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
